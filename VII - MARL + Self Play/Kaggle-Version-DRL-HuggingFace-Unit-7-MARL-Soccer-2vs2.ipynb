{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unit 7 : AI vs AI - Multi Agent Reinforcement Learning(MARL)","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit0/thumbnail.png\" alt=\"Thumbnail\"/>","metadata":{}},{"cell_type":"markdown","source":"**Full Tutorial**: https://huggingface.co/learn/deep-rl-course/unit7/hands-on","metadata":{}},{"cell_type":"markdown","source":"# What is AI vs. AI?","metadata":{}},{"cell_type":"markdown","source":"AI vs. AI is an open-source tool we developed at Hugging Face to compete agents on the Hub against one another in a multi-agent setting. These models are then ranked in a leaderboard.\n\nThe idea of this tool is to have a robust evaluation tool: **by evaluating your agent with a lot of others, you‚Äôll get a good idea of the quality of your policy.**\n\nMore precisely, AI vs. AI is three tools:\n\n* A **matchmaking process** defining the matches (which model against which) and running the model fights using a background task in the Space.\n* A **leaderboard** getting the match history results and displaying the models‚Äô ELO ratings: https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos\n* A **Space demo** to visualize your agents playing against others: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos","metadata":{}},{"cell_type":"markdown","source":"In addition to these three tools, your classmate cyllum created a ü§ó SoccerTwos Challenge Analytics where you can check the detailed match results of a model: https://huggingface.co/spaces/cyllum/soccertwos-analytics\n\nWe‚Äôre wrote [a blog post to explain this AI vs. AI tool in detail](https://huggingface.co/blog/aivsai), but to give you the big picture it works this way:\n\n* Every four hours, our algorithm fetches all the available models for a given environment (in our case ML-Agents-SoccerTwos).\n* It creates a queue of matches with the matchmaking algorithm.\n* We simulate the match in a Unity headless process and gather the match result (1 if the first model won, 0.5 if it‚Äôs a draw, 0 if the second model won) in a Dataset.\n* Then, when all matches from the matches queue are done, we update the ELO score for each model and update the leaderboard.","metadata":{}},{"cell_type":"markdown","source":"## Competition Rules\n\nThis first AI vs. AI competition **is an experiment**: the goal is to improve the tool in the future with your feedback. So **some breakups can happen during the challenge**. But don‚Äôt worry all the results are saved in a dataset so we can always restart the calculation correctly without losing information.\n\nIn order for your model to get correctly evaluated against others you need to follow these rules:\n\n* You can‚Äôt change the observation space or action space of the agent. By doing that your model will not work during evaluation.\n* You can‚Äôt use a custom trainer for now, you need to use the Unity MLAgents ones.\n* We provide executables to train your agents. You can also use the Unity Editor if you prefer , but to avoid bugs, we advise that you use our executables.\n\nWhat will make the difference during this challenge are **the hyperparameters you choose**.","metadata":{}},{"cell_type":"markdown","source":"# Step 0: Install MLAgents and download the correct executable","metadata":{}},{"cell_type":"code","source":"python_version = \"3.10.12\"\nenv_name = \"rl\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda create --name {env_name} python={python_version} -y","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!conda clean -ya","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nold_path = os.environ[\"PATH\"]\nnew_path = f\"/opt/conda/envs/{env_name}/bin:{old_path}\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%env PATH=$new_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\n\nimport sys\n\nif __name__ == \"__main__\":\n    print(sys.path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!/opt/conda/envs/{env_name}/bin/python ./main.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd ml-agents\n!pip install -e ./ml-agents-envs\n!pip install -e ./ml-agents","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that it‚Äôs installed, we need to add the environment training executable. Based on your operating system you need to download one of them, unzip it and place it in a new folder inside ml-agents that you call training-envs-executables\n\nAt the end your executable should be in ml-agents/training-envs-executables/SoccerTwos\n\n* Windows: Download this [executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view)\n* Linux: Download this [executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view)","metadata":{}},{"cell_type":"code","source":"!mkdir ./trained-envs-executables\n!mkdir ./trained-envs-executables/linux","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now unzip the file in linux folder. Also change the permission of the unzip file. Then you are godd to go","metadata":{}},{"cell_type":"code","source":"# import shutil\n# shutil.make_archive('results', 'zip', '/kaggle/input/soccertwos-files/results/results')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import shutil\n# src_path = r\"/kaggle/working/ml-agents/results.zip\"\n# dst_path = r\"/kaggle/working/ml-agents/trained-envs-executables/linux/SoccerTwos/UnityPlayer.so\"\n# shutil.copy(src_path, dst_path)\n# print('Copied')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !unzip -d /kaggle/working/results /kaggle/working/ml-agents/results.zip","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !chmod -R 755 /kaggle/working/ml-agents/trained-envs-executables/linux/SoccerTwos/SoccerTwos.x86_64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# src_path = \"/kaggle/working/results/SoccerTwos/SoccerTwos/SoccerTwos-3999868.pt\"\n# dst_path = \"/kaggle/working/results/SoccerTwos/SoccerTwos/checkpoint.pt\"\n# shutil.copy(src_path, dst_path)\n# print('Copied')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Understand the environment","metadata":{}},{"cell_type":"markdown","source":"The environment is called SoccerTwos. The Unity MLAgents Team made it. You can find its documentation [here](https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md#soccer-twos)\n\nThe goal in this environment is to get the ball into the opponent‚Äôs goal while preventing the ball from entering your own goal.\n\n\n‚¨áÔ∏è Here is an example of **2 vs 2 soccer agents playing.** ‚¨áÔ∏è\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccertwos.gif\" alt=\"Thumbnail\"/>","metadata":{}},{"cell_type":"markdown","source":"## Reward Function\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/soccerreward.png\" alt='reward-function' />","metadata":{}},{"cell_type":"markdown","source":"## Observation Space\n\nThe observation space is composed of vectors of size 336:\n\n* 11 ray-casts forward distributed over 120 degrees (264 state dimensions)\n* 3 ray-casts backward distributed over 90 degrees (72 state dimensions)\n* Both of these ray-casts can detect 6 objects:\n    * Ball\n    * Blue Goal\n    * Purple Goal\n    * Wall\n    * Blue Agent\n    * Purple Agent","metadata":{}},{"cell_type":"markdown","source":"## Action Space\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/socceraction.png\" alt=\"action-space\" />","metadata":{}},{"cell_type":"markdown","source":"# Step 2: Understand MA-POCA","metadata":{}},{"cell_type":"markdown","source":"We know how to train agents to play against others: we can use self-play. This is a perfect technique for a 1vs1.\n\nBut in our case we‚Äôre 2vs2, and each team has 2 agents. How then can we train cooperative behavior for groups of agents?\n\nAs explained in the [Unity Blog](https://blog.unity.com/technology/ml-agents-v20-release-now-supports-training-complex-cooperative-behaviors), agents typically receive a reward as a group (+1 - penalty) when the team scores a goal. This implies that every agent on the team is rewarded even if each agent didn‚Äôt contribute the same to the win, which makes it difficult to learn what to do independently.\n\nThe Unity MLAgents team developed the solution in a new multi-agent trainer called **MA-POCA (Multi-Agent POsthumous Credit Assignment)**.\n\nThe idea is simple but powerful: a centralized critic processes the states of all agents in the team to estimate how well each agent is doing. Think of this critic as a coach.\n\nThis allows each agent to make decisions based only on what it perceives locally, and simultaneously evaluate how good its behavior is in the context of the whole group.\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/mapoca.png\" alt=\"MA_POCA\"/>\n\nThe solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will help us to train cooperative behavior and self-play to win against an opponent team. Self-play with config params described [here](https://huggingface.co/learn/deep-rl-course/unit7/self-play)\n\nIf you want to dive deeper into this MA-POCA algorithm, you need to read the paper they published [here](https://arxiv.org/pdf/2111.05992.pdf) and the sources we put on the additional readings section.","metadata":{}},{"cell_type":"markdown","source":"# Step 3: Define Config File","metadata":{}},{"cell_type":"markdown","source":"Now, you define the training hyperparameters in config.yaml files.\n\nThere are multiple hyperparameters. To understand them better, you should read the explanations for each of them in the documentation\n\nThe config file we‚Äôre going to use here is in ***./config/poca/SoccerTwos.yaml***.\n\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How you modify them can be critical in getting good results.\n\nThe advice I can give you here is to check the explanation and recommended value for each parameters (especially self-play ones) against the [documentation](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md).\n\nNow that you‚Äôve modified our config file, you‚Äôre ready to train your agents.","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/ml-agents/config/poca/SoccerTwos.yaml\nbehaviors:\n  SoccerTwos:\n    trainer_type: poca\n    hyperparameters:\n      batch_size: 2048\n      buffer_size: 20480\n      learning_rate: 0.0005\n      beta: 0.005\n      epsilon: 0.2\n      lambd: 0.95\n      num_epoch: 3\n      learning_rate_schedule: linear\n      beta_schedule: linear\n      epsilon_schedule: linear\n    checkpoint_interval: 200000\n    network_settings:\n      normalize: false\n      hidden_units: 512\n      num_layers: 2\n      vis_encode_type: simple\n      memory: null\n      goal_conditioning_type: hyper\n      deterministic: false\n    reward_signals:\n      extrinsic:\n        gamma: 0.99\n        strength: 1.0\n        network_settings:\n          normalize: false\n          hidden_units: 128\n          num_layers: 2\n          vis_encode_type: simple\n          memory: null\n          goal_conditioning_type: hyper\n          deterministic: false\n    keep_checkpoints: 5\n    even_checkpoints: false\n    max_steps: 10000000\n    time_horizon: 1000\n    summary_freq: 10000\n    threaded: false\n    self_play:\n      save_steps: 50000\n      team_change: 250000\n      swap_steps: 2000\n      window: 10\n      play_against_latest_model_ratio: 0.5\n      initial_elo: 1200.0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Train Agent","metadata":{}},{"cell_type":"markdown","source":"To train the agents, we need to launch mlagents-learn and select the executable containing the environment.\n\nWe define *four* parameters:\n\n* mlagents-learn <config>: the path where the hyperparameter config file is.\n* -env: where the environment executable is.\n* -run_id: the name you want to give to your training run id.\n* -no-graphics: to not launch the visualization during the training.\n\nDepending on your hardware, 5M timesteps (the recommended value, but you can also try 10M) will take 5 to 8 hours of training. You can continue using your computer in the meantime, but I advise deactivating the computer standby mode to prevent the training from being stopped.\n\nDepending on the executable you use (windows, ubuntu, mac) the training command will look like this (your executable path can be different so don‚Äôt hesitate to check before running).\n\nThe executable contains 8 copies of SoccerTwos.\n\n‚ö†Ô∏è It‚Äôs normal if you don‚Äôt see a big increase of ELO score (and even a decrease below 1200) before 2M timesteps, since your agents will spend most of their time moving randomly on the field before being able to goal.\n\n‚ö†Ô∏è You can stop the training with Ctrl + C but beware of typing this command only once to stop the training since MLAgents needs to generate a final .onnx file before closing the run.","metadata":{}},{"cell_type":"code","source":"!mlagents-learn \"/kaggle/working/ml-agents/config/poca/SoccerTwos.yaml\" --env=\"/kaggle/working/ml-agents/trained-envs-executables/linux/SoccerTwos/SoccerTwos\" --run-id=\"SoccerTwos\" --no-graphics --resume","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Resume Training With Modified Config\n\nAfter you have modified the config or for some reasons, previous run terminated or stopped or you want to make your agent stronger. just add *--resume* at the end to continue training from the last saved agent","metadata":{}},{"cell_type":"markdown","source":"# Step 5: Push the agent to the Hugging Face Hub","metadata":{}},{"cell_type":"markdown","source":"Now that we trained our agents, we‚Äôre ready to push them to the Hub to be able to participate in the AI vs. AI challenge and visualize them playing on your browserüî•.\n\nTo be able to share your model with the community, there are three more steps to follow:\n\n1Ô∏è‚É£ (If it‚Äôs not already done) create an account to HF ‚û° https://huggingface.co/join\n\n2Ô∏è‚É£ Sign in and store your authentication token from the Hugging Face website.\n\nCreate a new token (https://huggingface.co/settings/tokens) with write role\n\nCopy the token, run this, and paste the token\n","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Then, we need to run mlagents-push-to-hf.\n\nAnd we define *four* parameters:\n\n* -run-id: the name of the training run id.\n* -local-dir: where the agent was saved, it‚Äôs *results/<run_id name>*, so in my case results/First Training.\n* -repo-id: the name of the Hugging Face repo you want to create or update. It‚Äôs always *<huggingface username / repo name>* If the repo does not exist it will be created automatically\n* --commit-message: since HF repos are git repositories you need to give a commit message.","metadata":{}},{"cell_type":"code","source":"!mlagents-push-to-hf  --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"hishamcse/poca-SoccerTwos\" --commit-message=\"First Push\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Verify that your model is ready for AI vs AI Challenge","metadata":{}},{"cell_type":"markdown","source":"Now that your model is pushed to the Hub, it‚Äôs going to be added automatically to the AI vs AI Challenge model pool. It can take a little bit of time before your model is added to the leaderboard given we do a run of matches every 4h.\n\nBut to ensure that everything works perfectly you need to check:\n\n* That you have this tag in your model: *ML-Agents-SoccerTwos*. This is the tag we use to select models to be added to the challenge pool. To do that go to your model and check the tags\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify1.png\" alt=\"tag\" />\n\nIf it‚Äôs not the case you just need to modify the readme and add it\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/verify2.png\" alt=\"readme\" />\n\n* That you have a SoccerTwos.onnx file\n\nWe strongly suggest that you create a new model when you push to the Hub if you want to train it again or train a new version.","metadata":{}},{"cell_type":"markdown","source":"# Step 7: Visualize some match in our demo","metadata":{}},{"cell_type":"markdown","source":"Now that your model is part of AI vs AI Challenge, you can visualize how good it is compared to others: https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos\n\nIn order to do that, you just need to go to this demo:\n\n* Select your model as team blue (or team purple if you prefer) and another model to compete against. The best opponents to compare your model to are either whoever is on top of the leaderboard or the [baseline](https://huggingface.co/unity/MLAgents-SoccerTwos) model\n\nThe matches you see live are not used in the calculation of your result but they are a good way to visualize how good your agent is.\n\nAnd don‚Äôt hesitate to share the best score your agent gets on discord in the #rl-i-made-this channel","metadata":{}}]}