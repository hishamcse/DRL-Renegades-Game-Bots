{"metadata":{"colab":{"private_outputs":true,"provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unit 8: Proximal Policy Gradient (PPO) with PyTorch ü§ñ\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png\" alt=\"Unit 8\"/>\n\n\nIn this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL implementation as model**.\n\nTo test its robustness, we're going to train it in:\n\n- [LunarLander-v2 üöÄ](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)\n","metadata":{"id":"-cf5-oDPjwf8"}},{"cell_type":"markdown","source":"‚¨áÔ∏è Here is an example of what you will achieve. ‚¨áÔ∏è","metadata":{"id":"2Fl6Rxt0lc0O"}},{"cell_type":"code","source":"%%html\n<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>","metadata":{"id":"DbKfCj5ilgqT","execution":{"iopub.status.busy":"2024-06-24T16:09:12.889592Z","iopub.execute_input":"2024-06-24T16:09:12.889922Z","iopub.status.idle":"2024-06-24T16:09:12.896886Z","shell.execute_reply.started":"2024-06-24T16:09:12.889897Z","shell.execute_reply":"2024-06-24T16:09:12.896033Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).","metadata":{"id":"YcOFdWpnlxNf"}},{"cell_type":"markdown","source":"## Objectives of this notebook üèÜ\n\nAt the end of the notebook, you will:\n\n- Be able to **code your PPO agent from scratch using PyTorch**.\n- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n\n\n","metadata":{"id":"T6lIPYFghhYL"}},{"cell_type":"markdown","source":"## This notebook is from the Deep Reinforcement Learning Course\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n\nIn this free course, you will:\n\n- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n- ü§ñ Train **agents in unique environments**\n\nDon‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n\n\nThe best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5","metadata":{"id":"Wp-rD6Fuhq31"}},{"cell_type":"markdown","source":"## Prerequisites üèóÔ∏è\nBefore diving into the notebook, you need to:\n\nüî≤ üìö Study [PPO by reading Unit 8](https://huggingface.co/deep-rl-course/unit8/introduction) ü§ó  ","metadata":{"id":"rasqqGQlhujA"}},{"cell_type":"markdown","source":"To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push one model, we don't ask for a minimal result but we **advise you to try different hyperparameters settings to get better results**.\n\nIf you don't find your model, **go to the bottom of the page and click on the refresh button**\n\nFor more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process","metadata":{"id":"PUFfMGOih3CW"}},{"cell_type":"markdown","source":"## Set the GPU üí™\n- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">","metadata":{"id":"PU4FVzaoM6fC"}},{"cell_type":"markdown","source":"- `Hardware Accelerator > GPU`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">","metadata":{"id":"KV0NyFdQM9ZG"}},{"cell_type":"markdown","source":"# Create a virtual display üîΩ\n\nDuring the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n\nHence the following cell will install the librairies and create and run a virtual screen üñ•","metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","source":"!pip install setuptools==65.5.0","metadata":{"id":"Fd731S8-NuJA","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!apt install python-opengl -y\n!apt install ffmpeg\n!apt install xvfb\n!apt install swig cmake\n!pip install pyglet==1.5\n!pip3 install pyvirtualdisplay","metadata":{"id":"jV6wjQ7Be7p5","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"id":"ww5PQH1gNLI4","execution":{"iopub.status.busy":"2024-06-24T16:16:19.684758Z","iopub.execute_input":"2024-06-24T16:16:19.685484Z","iopub.status.idle":"2024-06-24T16:16:20.039542Z","shell.execute_reply.started":"2024-06-24T16:16:19.685433Z","shell.execute_reply":"2024-06-24T16:16:20.038497Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<pyvirtualdisplay.display.Display at 0x7ad048c9beb0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Install dependencies üîΩ\nFor this exercise, we use `gym==0.22`.","metadata":{"id":"ncIgfNf3mOtc"}},{"cell_type":"code","source":"!pip install gym==0.22\n!pip install imageio-ffmpeg\n!pip install huggingface_hub\n!pip install gym[box2d]==0.22","metadata":{"id":"9xZQFTPcsKUK","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's code Normal PPO from scratch with Costa Huang tutorial\n- For the core implementation of PPO we're going to use the excellent [Costa Huang](https://costa.sh/) tutorial.\n- In addition to the tutorial, to go deeper you can read the 37 core implementation details: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n- The full Code implementation of this blog: https://github.com/vwxyzjn/ppo-implementation-details\n- Documentation of this implementation: https://docs.cleanrl.dev/rl-algorithms/ppo/\n- For All Algorithms Single File Implementation: https://github.com/vwxyzjn/cleanrl\n\nüëâ The video tutorial: https://youtu.be/MEt6rrxH8W4","metadata":{"id":"oDkUufewmq6v"}},{"cell_type":"markdown","source":"## Import ","metadata":{}},{"cell_type":"code","source":"%%writefile ppo.py\nimport argparse\nimport os\nimport random\nimport time\nfrom distutils.util import strtobool\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.distributions.categorical import Categorical\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom huggingface_hub import HfApi, upload_folder\nfrom huggingface_hub.repocard import metadata_eval_result, metadata_save\n\nfrom pathlib import Path\nimport datetime\nimport tempfile\nimport json\nimport shutil\nimport imageio\n\nfrom wasabi import Printer\nmsg = Printer()","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:28.517805Z","iopub.execute_input":"2024-06-24T16:25:28.518851Z","iopub.status.idle":"2024-06-24T16:25:28.525934Z","shell.execute_reply.started":"2024-06-24T16:25:28.518807Z","shell.execute_reply":"2024-06-24T16:25:28.524956Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Overwriting ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Argument Parser","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef parse_args():\n    # fmt: off\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--exp-name\", type=str, default=os.path.basename(__file__).rstrip(\".py\"),\n        help=\"the name of this experiment\")\n    parser.add_argument(\"--seed\", type=int, default=1,\n        help=\"seed of the experiment\")\n    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"if toggled, cuda will be enabled by default\")\n    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n        help=\"the wandb's project name\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n        help=\"the entity (team) of wandb's project\")\n    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n        help=\"whether to capture videos of the agent performances (check out `videos` folder)\")\n\n    # Algorithm specific arguments\n    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n        help=\"the id of the environment\")\n    parser.add_argument(\"--total-timesteps\", type=int, default=50000,\n        help=\"total timesteps of the experiments\")\n    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n        help=\"the learning rate of the optimizer\")\n    parser.add_argument(\"--num-envs\", type=int, default=4,\n        help=\"the number of parallel game environments\")\n    parser.add_argument(\"--num-steps\", type=int, default=128,\n        help=\"the number of steps to run in each environment per policy rollout\")\n    parser.add_argument(\"--anneal-lr\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggle learning rate annealing for policy and value networks\")\n    parser.add_argument(\"--gae\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Use GAE for advantage computation\")\n    parser.add_argument(\"--gamma\", type=float, default=0.99,\n        help=\"the discount factor gamma\")\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95,\n        help=\"the lambda for the general advantage estimation\")\n    parser.add_argument(\"--num-minibatches\", type=int, default=4,\n        help=\"the number of mini-batches\")\n    parser.add_argument(\"--update-epochs\", type=int, default=4,\n        help=\"the K epochs to update the policy\")\n    parser.add_argument(\"--norm-adv\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles advantages normalization\")\n    parser.add_argument(\"--clip-coef\", type=float, default=0.2,\n        help=\"the surrogate clipping coefficient\")\n    parser.add_argument(\"--clip-vloss\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n        help=\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\")\n    parser.add_argument(\"--ent-coef\", type=float, default=0.01,\n        help=\"coefficient of the entropy\")\n    parser.add_argument(\"--vf-coef\", type=float, default=0.5,\n        help=\"coefficient of the value function\")\n    parser.add_argument(\"--max-grad-norm\", type=float, default=0.5,\n        help=\"the maximum norm for the gradient clipping\")\n    parser.add_argument(\"--target-kl\", type=float, default=0.015,\n        help=\"the target KL divergence threshold\")\n\n    # Adding HuggingFace argument\n    parser.add_argument(\"--repo-id\", type=str, default=\"hishamcse/ppo-CartPole-v1-scratch\", help=\"id of the model repository from the Hugging Face Hub {username/repo_name}\")\n\n    args = parser.parse_args()\n    args.batch_size = int(args.num_envs * args.num_steps)\n    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n    # fmt: on\n    return args","metadata":{"id":"_bE708C6mhE7","execution":{"iopub.status.busy":"2024-06-24T16:25:31.350855Z","iopub.execute_input":"2024-06-24T16:25:31.351210Z","iopub.status.idle":"2024-06-24T16:25:31.359425Z","shell.execute_reply.started":"2024-06-24T16:25:31.351180Z","shell.execute_reply":"2024-06-24T16:25:31.358408Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Make Environment ","metadata":{}},{"cell_type":"markdown","source":"Implements **vectorized architecture** of the environment","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef make_env(env_id, seed, idx, capture_video, run_name):\n    def thunk():\n        env = gym.make(env_id)\n        env = gym.wrappers.RecordEpisodeStatistics(env)\n        if capture_video:\n            if idx == 0:\n                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n        env.seed(seed)\n        env.action_space.seed(seed)\n        env.observation_space.seed(seed)\n        return env\n\n    return thunk","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:35.388353Z","iopub.execute_input":"2024-06-24T16:25:35.389201Z","iopub.status.idle":"2024-06-24T16:25:35.394907Z","shell.execute_reply.started":"2024-06-24T16:25:35.389168Z","shell.execute_reply":"2024-06-24T16:25:35.393935Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Agent Setup ","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n    torch.nn.init.orthogonal_(layer.weight, std)\n    torch.nn.init.constant_(layer.bias, bias_const)\n    return layer","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:38.393437Z","iopub.execute_input":"2024-06-24T16:25:38.394061Z","iopub.status.idle":"2024-06-24T16:25:38.399597Z","shell.execute_reply.started":"2024-06-24T16:25:38.394028Z","shell.execute_reply":"2024-06-24T16:25:38.398689Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile -a ppo.py\n\nclass Agent(nn.Module):\n    def __init__(self, envs):\n        super().__init__()\n        self.critic = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 1), std=1.0),\n        )\n        self.actor = nn.Sequential(\n            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, 64)),\n            nn.Tanh(),\n            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n        )\n        \n    def get_value(self, x):\n        return self.critic(x)\n\n    def get_action_and_value(self, x, action=None):\n        logits = self.actor(x)\n        probs = Categorical(logits=logits)\n        if action is None:\n            action = probs.sample()\n        return action, probs.log_prob(action), probs.entropy(), self.critic(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:40.920545Z","iopub.execute_input":"2024-06-24T16:25:40.920941Z","iopub.status.idle":"2024-06-24T16:25:40.927570Z","shell.execute_reply.started":"2024-06-24T16:25:40.920912Z","shell.execute_reply":"2024-06-24T16:25:40.926448Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Add the Hugging Face Integration ü§ó\n- In order to push our model to the Hub, we need to define a function `package_to_hub`\n- Next, we add the methods needed to push the model to the Hub\n\n- These methods will:\n  - `_evalutate_agent()`: evaluate the agent.\n  - `_generate_model_card()`: generate the model card of your agent.\n  - `_record_video()`: record a video of your agent.","metadata":{"id":"mk-a9CmNuS2W"}},{"cell_type":"markdown","source":"### Evaluate Agent ","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef _evaluate_agent(env, n_eval_episodes, policy):\n  \"\"\"\n  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n  :param env: The evaluation environment\n  :param n_eval_episodes: Number of episode to evaluate the agent\n  :param policy: The agent\n  \"\"\"\n  episode_rewards = []\n  for episode in range(n_eval_episodes):\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards_ep = 0\n\n    while done is False:\n      state = torch.Tensor(state).to(device)\n      action, _, _, _ = policy.get_action_and_value(state)\n      new_state, reward, done, info = env.step(action.cpu().numpy())\n        \n      total_rewards_ep += reward\n      if done:\n        break\n      state = new_state\n    episode_rewards.append(total_rewards_ep)\n  mean_reward = np.mean(episode_rewards)\n  std_reward = np.std(episode_rewards)\n\n  return mean_reward, std_reward","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:45.112807Z","iopub.execute_input":"2024-06-24T16:25:45.113168Z","iopub.status.idle":"2024-06-24T16:25:45.119383Z","shell.execute_reply.started":"2024-06-24T16:25:45.113138Z","shell.execute_reply":"2024-06-24T16:25:45.118401Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Record Video ","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef record_video(env, policy, out_directory, fps=30):\n  images = []\n  done = False\n  state = env.reset()\n  img = env.render(mode='rgb_array')\n  images.append(img)\n  while not done:\n    state = torch.Tensor(state).to(device)\n    # Take the action (index) that have the maximum expected future reward given that state\n    action, _, _, _  = policy.get_action_and_value(state)\n    state, reward, done, info = env.step(action.cpu().numpy()) # We directly put next_state = state for recording logic\n    img = env.render(mode='rgb_array')\n    images.append(img)\n  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:48.176345Z","iopub.execute_input":"2024-06-24T16:25:48.176717Z","iopub.status.idle":"2024-06-24T16:25:48.183826Z","shell.execute_reply.started":"2024-06-24T16:25:48.176686Z","shell.execute_reply":"2024-06-24T16:25:48.182899Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Generate Model Card & Metadata ","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n  \"\"\"\n  Generate the model card for the Hub\n  :param model_name: name of the model\n  :env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  :hyperparameters: training arguments\n  \"\"\"\n  # Step 1: Select the tags\n  metadata = generate_metadata(model_name, env_id, mean_reward, std_reward)\n\n  # Transform the hyperparams namespace to string\n  converted_dict = vars(hyperparameters)\n  converted_str = str(converted_dict)\n  converted_str = converted_str.split(\", \")\n  converted_str = '\\n'.join(converted_str)\n\n  # Step 2: Generate the model card\n  model_card = f\"\"\"\n  # PPO Agent Implemented from scratch Playing {env_id}\n\n  This is a trained model of a PPO agent playing {env_id}.\n\n  # Hyperparameters\n  ```python\n  {converted_str}\n  ```\n  \"\"\"\n  return model_card, metadata\n\n\ndef generate_metadata(model_name, env_id, mean_reward, std_reward):\n  \"\"\"\n  Define the tags for the model card\n  :param model_name: name of the model\n  :param env_id: name of the environment\n  :mean_reward: mean reward of the agent\n  :std_reward: standard deviation of the mean reward of the agent\n  \"\"\"\n  metadata = {}\n  metadata[\"tags\"] = [\n        env_id,\n        \"ppo\",\n        \"deep-reinforcement-learning\",\n        \"reinforcement-learning\",\n        \"scratch-implementation\",\n        \"deep-rl-course\"\n  ]\n\n  # Add metrics\n  eval = metadata_eval_result(\n      model_pretty_name=model_name,\n      task_pretty_name=\"reinforcement-learning\",\n      task_id=\"reinforcement-learning\",\n      metrics_pretty_name=\"mean_reward\",\n      metrics_id=\"mean_reward\",\n      metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n      dataset_pretty_name=env_id,\n      dataset_id=env_id,\n  )\n\n  # Merges both dictionaries\n  metadata = {**metadata, **eval}\n\n  return metadata\n\n\ndef _save_model_card(local_path, generated_model_card, metadata):\n    \"\"\"Saves a model card for the repository.\n    :param local_path: repository directory\n    :param generated_model_card: model card generated by _generate_model_card()\n    :param metadata: metadata\n    \"\"\"\n    readme_path = local_path / \"README.md\"\n    readme = \"\"\n    if readme_path.exists():\n        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n            readme = f.read()\n    else:\n        readme = generated_model_card\n\n    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(readme)\n\n    # Save our metrics to Readme metadata\n    metadata_save(readme_path, metadata)\n\n\ndef _add_logdir(local_path: Path, logdir: Path):\n  \"\"\"Adds a logdir to the repository.\n  :param local_path: repository directory\n  :param logdir: logdir directory\n  \"\"\"\n  if logdir.exists() and logdir.is_dir():\n    # Add the logdir to the repository under new dir called logs\n    repo_logdir = local_path / \"logs\"\n\n    # Delete current logs if they exist\n    if repo_logdir.exists():\n      shutil.rmtree(repo_logdir)\n\n    # Copy logdir into repo logdir\n    shutil.copytree(logdir, repo_logdir)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:25:51.268630Z","iopub.execute_input":"2024-06-24T16:25:51.269003Z","iopub.status.idle":"2024-06-24T16:25:51.276449Z","shell.execute_reply.started":"2024-06-24T16:25:51.268976Z","shell.execute_reply":"2024-06-24T16:25:51.275528Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Push to hub ","metadata":{}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\ndef package_to_hub(repo_id,\n                model,\n                hyperparameters,\n                eval_env,\n                video_fps=30,\n                commit_message=\"Push agent to the Hub\",\n                token= None,\n                logs=None\n                ):\n  \"\"\"\n  Evaluate, Generate a video and Upload a model to Hugging Face Hub.\n  This method does the complete pipeline:\n  - It evaluates the model\n  - It generates the model card\n  - It generates a replay video of the agent\n  - It pushes everything to the hub\n  :param repo_id: id of the model repository from the Hugging Face Hub\n  :param model: trained model\n  :param eval_env: environment used to evaluate the agent\n  :param fps: number of fps for rendering the video\n  :param commit_message: commit message\n  :param logs: directory on local machine of tensorboard logs you'd like to upload\n  \"\"\"\n  msg.info(\n        \"This function will save, evaluate, generate a video of your agent, \"\n        \"create a model card and push everything to the hub. \"\n        \"It might take up to 1min. \\n \"\n        \"This is a work in progress: if you encounter a bug, please open an issue.\"\n    )\n  # Step 1: Clone or create the repo\n  repo_url = HfApi().create_repo(\n        repo_id=repo_id,\n        token=token,\n        private=False,\n        exist_ok=True,\n    )\n\n  with tempfile.TemporaryDirectory() as tmpdirname:\n    tmpdirname = Path(tmpdirname)\n\n    # Step 2: Save the model\n    torch.save(model.state_dict(), tmpdirname / \"model.pt\")\n\n    # Step 3: Evaluate the model and build JSON\n    mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n\n    # First get datetime\n    eval_datetime = datetime.datetime.now()\n    eval_form_datetime = eval_datetime.isoformat()\n\n    evaluate_data = {\n        \"env_id\": hyperparameters.env_id,\n        \"mean_reward\": mean_reward,\n        \"std_reward\": std_reward,\n        \"n_evaluation_episodes\": 10,\n        \"eval_datetime\": eval_form_datetime,\n    }\n\n    # Write a JSON file\n    with open(tmpdirname / \"results.json\", \"w\") as outfile:\n      json.dump(evaluate_data, outfile)\n\n    # Step 4: Generate a video\n    video_path =  tmpdirname / \"replay.mp4\"\n    record_video(eval_env, model, video_path, video_fps)\n\n    # Step 5: Generate the model card\n    generated_model_card, metadata = _generate_model_card(\"PPO\", hyperparameters.env_id, mean_reward, std_reward, hyperparameters)\n    _save_model_card(tmpdirname, generated_model_card, metadata)\n\n    # Step 6: Add logs if needed\n    if logs:\n      _add_logdir(tmpdirname, Path(logs))\n\n    msg.info(f\"Pushing repo {repo_id} to the Hugging Face Hub\")\n\n    repo_url = upload_folder(\n            repo_id=repo_id,\n            folder_path=tmpdirname,\n            path_in_repo=\"\",\n            commit_message=commit_message,\n            token=token,\n        )\n\n    msg.info(f\"Your model is pushed to the Hub. You can view your model here: {repo_url}\")\n  return repo_url","metadata":{"id":"WlLcz4L9odXs","execution":{"iopub.status.busy":"2024-06-24T16:25:55.876521Z","iopub.execute_input":"2024-06-24T16:25:55.876917Z","iopub.status.idle":"2024-06-24T16:25:55.884637Z","shell.execute_reply.started":"2024-06-24T16:25:55.876891Z","shell.execute_reply":"2024-06-24T16:25:55.883665Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Main: Train, Evaluate & Publish ","metadata":{}},{"cell_type":"markdown","source":"### Clipped Value Loss Function\n\n![image.png](attachment:70c52b87-669a-489a-be79-2d79fc931c61.png)","metadata":{},"attachments":{"70c52b87-669a-489a-be79-2d79fc931c61.png":{"image/png":"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"}}},{"cell_type":"code","source":"%%writefile -a ppo.py\n\nif __name__ == \"__main__\":\n    args = parse_args()\n    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n    if args.track:\n        import wandb\n\n        wandb.init(\n            project=args.wandb_project_name,\n            entity=args.wandb_entity,\n            sync_tensorboard=True,\n            config=vars(args),\n            name=run_name,\n            monitor_gym=True,\n            save_code=True,\n        )\n        \n    writer = SummaryWriter(f\"runs/{run_name}\")\n    writer.add_text(\n        \"hyperparameters\",\n        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n    )\n    \n    # TRY NOT TO MODIFY: seeding\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    torch.backends.cudnn.deterministic = args.torch_deterministic\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n    \n    # env setup\n    envs = gym.vector.SyncVectorEnv(\n        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n    )\n    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n\n    # agent setup\n    agent = Agent(envs).to(device)\n    \n    # optimizer setup\n    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n    \n    # ALGO Logic: Storage setup\n    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n\n    # TRY NOT TO MODIFY: start the game\n    global_step = 0\n    start_time = time.time()\n    next_obs = torch.Tensor(envs.reset()).to(device)\n    next_done = torch.zeros(args.num_envs).to(device)\n    num_updates = args.total_timesteps // args.batch_size\n    \n    for update in range(1, num_updates + 1):\n        # Annealing the rate if instructed to do so.\n        if args.anneal_lr:\n            frac = 1.0 - (update - 1.0) / num_updates\n            lrnow = frac * args.learning_rate\n            optimizer.param_groups[0][\"lr\"] = lrnow\n\n        for step in range(0, args.num_steps):\n            global_step += 1 * args.num_envs\n            obs[step] = next_obs\n            dones[step] = next_done\n\n            # ALGO LOGIC: action logic\n            with torch.no_grad():\n                action, logprob, _, value = agent.get_action_and_value(next_obs)\n                values[step] = value.flatten()\n            actions[step] = action\n            logprobs[step] = logprob\n\n            # TRY NOT TO MODIFY: execute the game and log data.\n            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n            \n            rewards[step] = torch.tensor(reward).to(device).view(-1)\n            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n\n            for item in info:\n                if \"episode\" in item.keys():\n                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n                    break\n                    \n        # bootstrap value if not done\n        with torch.no_grad():\n            next_value = agent.get_value(next_obs).reshape(1, -1)\n            if args.gae:\n                advantages = torch.zeros_like(rewards).to(device)\n                lastgaelam = 0\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        nextvalues = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        nextvalues = values[t + 1]\n                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n                returns = advantages + values\n            else:\n                returns = torch.zeros_like(rewards).to(device)\n                for t in reversed(range(args.num_steps)):\n                    if t == args.num_steps - 1:\n                        nextnonterminal = 1.0 - next_done\n                        next_return = next_value\n                    else:\n                        nextnonterminal = 1.0 - dones[t + 1]\n                        next_return = returns[t + 1]\n                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n                advantages = returns - values\n\n        # flatten the batch\n        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n        b_logprobs = logprobs.reshape(-1)\n        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n        b_advantages = advantages.reshape(-1)\n        b_returns = returns.reshape(-1)\n        b_values = values.reshape(-1)\n\n        # Optimizing the policy and value network\n        b_inds = np.arange(args.batch_size)\n        clipfracs = []\n        for epoch in range(args.update_epochs):\n            np.random.shuffle(b_inds)\n            for start in range(0, args.batch_size, args.minibatch_size):\n                end = start + args.minibatch_size\n                mb_inds = b_inds[start:end]\n\n                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n                logratio = newlogprob - b_logprobs[mb_inds]\n                ratio = logratio.exp()\n                \n                # debug (approximate Kullback‚ÄìLeibler divergence)\n                with torch.no_grad():\n                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n                    old_approx_kl = (-logratio).mean()\n                    approx_kl = ((ratio - 1) - logratio).mean()     # new poposed in blog\n                    # the fraction of the training data that triggered the clipped objective.\n                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n\n                mb_advantages = b_advantages[mb_inds]\n                if args.norm_adv:\n                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n                    \n                # Policy loss (Clipped Surrogate Objective Function)\n                pg_loss1 = -mb_advantages * ratio     # the unclipper part\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)   # the clipped part\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n                \n                # Value loss (Clipped Value Loss Function)\n                newvalue = newvalue.view(-1)\n                if args.clip_vloss:\n                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n                    v_clipped = b_values[mb_inds] + torch.clamp(\n                        newvalue - b_values[mb_inds],\n                        -args.clip_coef,\n                        args.clip_coef,\n                    )\n                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n                    v_loss = 0.5 * v_loss_max.mean()\n                else:\n                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n                    \n                # Entropy loss\n                entropy_loss = entropy.mean()\n                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n                \n                # Global gradient clipping\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n                optimizer.step()\n            \n            # Early Stopping(Kullback‚ÄìLeibler divergence optimization)\n            if args.target_kl is not None:\n                if approx_kl > args.target_kl:\n                    break\n                    \n        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n        var_y = np.var(y_true)\n        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n\n        # TRY NOT TO MODIFY: record rewards for plotting purposes\n        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n        writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n        writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n        print(\"SPS:\", int(global_step / (time.time() - start_time)))\n        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n\n    envs.close()\n    writer.close()\n\n    # Create the evaluation environment\n    eval_env = gym.make(args.env_id)\n\n    package_to_hub(repo_id = args.repo_id,\n                model = agent, # The model we want to save\n                hyperparameters = args,\n                eval_env = gym.make(args.env_id),\n                logs= f\"runs/{run_name}\",\n                )","metadata":{"execution":{"iopub.status.busy":"2024-06-24T16:26:01.868855Z","iopub.execute_input":"2024-06-24T16:26:01.869218Z","iopub.status.idle":"2024-06-24T16:26:01.881488Z","shell.execute_reply.started":"2024-06-24T16:26:01.869189Z","shell.execute_reply":"2024-06-24T16:26:01.880553Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Appending to ppo.py\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## HuggingFace Login","metadata":{}},{"cell_type":"markdown","source":"To be able to share your model with the community there are three more steps to follow:\n\n1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n\n2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n\n- Copy the token\n- Run the cell below and paste the token","metadata":{"id":"JquRrWytA6eo"}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()\n!git config --global credential.helper store","metadata":{"id":"GZiFBBlzxzxY","execution":{"iopub.status.busy":"2024-06-24T16:21:50.859757Z","iopub.execute_input":"2024-06-24T16:21:50.860125Z","iopub.status.idle":"2024-06-24T16:21:52.075460Z","shell.execute_reply.started":"2024-06-24T16:21:50.860099Z","shell.execute_reply":"2024-06-24T16:21:52.074357Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41808215f5c94f4eb769ba22da9c8bcc"}},"metadata":{}}]},{"cell_type":"markdown","source":"If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`","metadata":{"id":"_tsf2uv0g_4p"}},{"cell_type":"markdown","source":"## Let's start the training üî•\n- ‚ö†Ô∏è ‚ö†Ô∏è ‚ö†Ô∏è  Don't use **the same repo id with the one you used for the Unit 1**\n- Now that you've coded from scratch PPO and added the Hugging Face Integration, we're ready to start the training üî•\n- Now we just need to run this python script using `python <name-of-python-script>.py` with the additional parameters we defined with `argparse`\n\n- You should modify more hyperparameters otherwise the training will not be super stable.","metadata":{"id":"jRqkGvk7pFQ6"}},{"cell_type":"code","source":"!python ppo.py --env-id=\"LunarLander-v2\" --repo-id=\"hishamcse/ppo-LunarLander-v2-scratch\" --total-timesteps=100000","metadata":{"id":"KXLih6mKseBs","_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See you on Unit 8, part 2 where we going to train agents to play Doom üî•\n## Keep learning, stay awesome ü§ó","metadata":{"id":"nYdl758GqLXT"}}]}