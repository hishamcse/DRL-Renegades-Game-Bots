{"metadata":{"colab":{"provenance":[],"private_outputs":true,"collapsed_sections":["tF42HvI7-gs5"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","gpuClass":"standard","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unit 6: Advantage Actor Critic (A2C) using Robotics Simulations with Panda-Gym ü§ñ\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/thumbnail.png\"  alt=\"Thumbnail\"/>\n\nIn this notebook, you'll learn to use A2C with [Panda-Gym](https://github.com/qgallouedec/panda-gym). You're going **to train a robotic arm** (Franka Emika Panda robot) to perform a task:\n\n- `Reach`: the robot must place its end-effector at a target position.\n\nAfter that, you'll be able **to train in other robotics tasks**.\n","metadata":{"id":"-PTReiOw-RAN"}},{"cell_type":"markdown","source":"### üéÆ Environments:\n\n- [Panda-Gym](https://github.com/qgallouedec/panda-gym)\n\n###üìö RL-Library:\n\n- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)","metadata":{"id":"QInFitfWno1Q"}},{"cell_type":"markdown","source":"We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).","metadata":{"id":"2CcdX4g3oFlp"}},{"cell_type":"markdown","source":"## Objectives of this notebook üèÜ\n\nAt the end of the notebook, you will:\n\n- Be able to use **Panda-Gym**, the environment library.\n- Be able to **train robots using A2C**.\n- Understand why **we need to normalize the input**.\n- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score üî•.\n\n\n","metadata":{"id":"MoubJX20oKaQ"}},{"cell_type":"markdown","source":"## This notebook is from the Deep Reinforcement Learning Course\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>\n\nIn this free course, you will:\n\n- üìñ Study Deep Reinforcement Learning in **theory and practice**.\n- üßë‚Äçüíª Learn to **use famous Deep RL libraries** such as Stable Baselines3, RL Baselines3 Zoo, CleanRL and Sample Factory 2.0.\n- ü§ñ Train **agents in unique environments**\n\nAnd more check üìö the syllabus üëâ https://simoninithomas.github.io/deep-rl-course\n\nDon‚Äôt forget to **<a href=\"http://eepurl.com/ic5ZUD\">sign up to the course</a>** (we are collecting your email to be able to¬†**send you the links when each Unit is published and give you information about the challenges and updates).**\n\n\nThe best way to keep in touch is to join our discord server to exchange with the community and with us üëâüèª https://discord.gg/ydHrjt3WP5","metadata":{"id":"DoUNkTExoUED"}},{"cell_type":"markdown","source":"## Prerequisites üèóÔ∏è\nBefore diving into the notebook, you need to:\n\nüî≤ üìö Study [Actor-Critic methods by reading Unit 6](https://huggingface.co/deep-rl-course/unit6/introduction) ü§ó  ","metadata":{"id":"BTuQAUAPoa5E"}},{"cell_type":"markdown","source":"# Let's train our first robots ü§ñ","metadata":{"id":"iajHvVDWoo01"}},{"cell_type":"markdown","source":"To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process),  you need to push your trained model to the Hub and get the following results:\n\n- `PandaReachDense-v3` get a result of >= -3.5.\n\nTo find your result, go to the [leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) and find your model, **the result = mean_reward - std of reward**\n\nFor more information about the certification process, check this section üëâ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process","metadata":{"id":"zbOENTE2os_D"}},{"cell_type":"markdown","source":"## Set the GPU üí™\n- To **accelerate the agent's training, we'll use a GPU**. To do that, go to `Runtime > Change Runtime type`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">","metadata":{"id":"PU4FVzaoM6fC"}},{"cell_type":"markdown","source":"- `Hardware Accelerator > GPU`\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">","metadata":{"id":"KV0NyFdQM9ZG"}},{"cell_type":"markdown","source":"## Create a virtual display üîΩ\n\nDuring the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n\nHence the following cell will install the librairies and create and run a virtual screen üñ•","metadata":{"id":"bTpYcVZVMzUI"}},{"cell_type":"code","source":"!apt install swig cmake\n!pip install swig\n\n!apt install -y python-opengl\n!apt install ffmpeg\n!apt install xvfb\n!pip3 install pyvirtualdisplay\n\n!pip install pyglet==1.5.1\n\n!pip install moviepy==1.0.3","metadata":{"id":"jV6wjQ7Be7p5","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-21T12:29:13.946286Z","iopub.execute_input":"2024-06-21T12:29:13.946681Z","iopub.status.idle":"2024-06-21T12:30:54.556981Z","shell.execute_reply.started":"2024-06-21T12:29:13.946646Z","shell.execute_reply":"2024-06-21T12:30:54.555457Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Virtual display\nfrom pyvirtualdisplay import Display\n\nvirtual_display = Display(visible=0, size=(1400, 900))\nvirtual_display.start()","metadata":{"id":"ww5PQH1gNLI4","execution":{"iopub.status.busy":"2024-06-21T12:31:54.609512Z","iopub.execute_input":"2024-06-21T12:31:54.610147Z","iopub.status.idle":"2024-06-21T12:31:55.091249Z","shell.execute_reply.started":"2024-06-21T12:31:54.610093Z","shell.execute_reply":"2024-06-21T12:31:55.089908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Install dependencies üîΩ\n\nThe first step is to install the dependencies, we‚Äôll install multiple ones:\n- `gymnasium`\n- `panda-gym`: Contains the robotics arm environments.\n- `stable-baselines3`: The SB3 deep reinforcement learning library.\n- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ü§ó Hub.\n- `huggingface_hub`: Library allowing anyone to work with the Hub repositories.\n\n‚è≤ The installation can **take 10 minutes**.","metadata":{"id":"e1obkbdJ_KnG"}},{"cell_type":"code","source":"!pip install stable-baselines3[extra]\n!pip install gymnasium","metadata":{"id":"TgZUkjKYSgvn","execution":{"iopub.status.busy":"2024-06-21T12:31:58.696146Z","iopub.execute_input":"2024-06-21T12:31:58.696563Z","iopub.status.idle":"2024-06-21T12:32:44.695232Z","shell.execute_reply.started":"2024-06-21T12:31:58.696530Z","shell.execute_reply":"2024-06-21T12:32:44.694030Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install huggingface_sb3\n!pip install huggingface_hub\n!pip install panda_gym","metadata":{"id":"ABneW6tOSpyU","execution":{"iopub.status.busy":"2024-06-21T12:33:04.200060Z","iopub.execute_input":"2024-06-21T12:33:04.200547Z","iopub.status.idle":"2024-06-21T12:33:48.948370Z","shell.execute_reply.started":"2024-06-21T12:33:04.200501Z","shell.execute_reply":"2024-06-21T12:33:48.946230Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import the packages üì¶","metadata":{"id":"QTep3PQQABLr"}},{"cell_type":"code","source":"import os\n\nimport gymnasium as gym\nimport panda_gym\n\nfrom huggingface_sb3 import load_from_hub, package_to_hub\n\nfrom stable_baselines3 import A2C\nfrom stable_baselines3.common.evaluation import evaluate_policy\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\nfrom stable_baselines3.common.env_util import make_vec_env\n\nfrom huggingface_hub import notebook_login","metadata":{"id":"HpiB8VdnQ7Bk","execution":{"iopub.status.busy":"2024-06-21T12:34:54.408364Z","iopub.execute_input":"2024-06-21T12:34:54.408777Z","iopub.status.idle":"2024-06-21T12:35:15.099330Z","shell.execute_reply.started":"2024-06-21T12:34:54.408743Z","shell.execute_reply":"2024-06-21T12:35:15.097909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PandaReachDense-v3 ü¶æ\n\nThe agent we're going to train is a robotic arm that needs to do controls (moving the arm and using the end-effector).\n\nIn robotics, the *end-effector* is the device at the end of a robotic arm designed to interact with the environment.\n\nIn `PandaReach`, the robot must place its end-effector at a target position (green ball).\n\nWe're going to use the dense version of this environment. It means we'll get a *dense reward function* that **will provide a reward at each timestep** (the closer the agent is to completing the task, the higher the reward). Contrary to a *sparse reward function* where the environment **return a reward if and only if the task is completed**.\n\nAlso, we're going to use the *End-effector displacement control*, it means the **action corresponds to the displacement of the end-effector**. We don't control the individual motion of each joint (joint control).\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/robotics.jpg\"  alt=\"Robotics\"/>\n\n\nThis way **the training will be easier**.\n\n","metadata":{"id":"lfBwIS_oAVXI"}},{"cell_type":"markdown","source":"### Create the environment\n\n#### The environment üéÆ\n\nIn `PandaReachDense-v3` the robotic arm must place its end-effector at a target position (green ball).","metadata":{"id":"frVXOrnlBerQ"}},{"cell_type":"code","source":"env_id = \"PandaReachDense-v3\"\n\n# Create the env\nenv = gym.make(env_id, render_mode='rgb_array')\n\n# Get the state space and action space\ns_size = env.observation_space.shape\na_size = env.action_space","metadata":{"id":"zXzAu3HYF1WD","execution":{"iopub.status.busy":"2024-06-21T16:04:10.497739Z","iopub.execute_input":"2024-06-21T16:04:10.498202Z","iopub.status.idle":"2024-06-21T16:04:10.628051Z","shell.execute_reply.started":"2024-06-21T16:04:10.498170Z","shell.execute_reply":"2024-06-21T16:04:10.626936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"_____OBSERVATION SPACE_____ \\n\")\nprint(\"The State Space is: \", s_size)\nprint(\"Sample observation\", env.observation_space.sample()) # Get a random observation","metadata":{"id":"E-U9dexcF-FB","execution":{"iopub.status.busy":"2024-06-21T12:36:42.285795Z","iopub.execute_input":"2024-06-21T12:36:42.286229Z","iopub.status.idle":"2024-06-21T12:36:42.296410Z","shell.execute_reply.started":"2024-06-21T12:36:42.286193Z","shell.execute_reply":"2024-06-21T12:36:42.294976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The observation space **is a dictionary with 3 different elements**:\n- `achieved_goal`: (x,y,z) the current position of the end-effector.\n- `desired_goal`: (x,y,z) the target position for the end-effector.\n- `observation`: position (x,y,z) and velocity of the end-effector (vx, vy, vz).\n\nGiven it's a dictionary as observation, **we will need to use a MultiInputPolicy policy instead of MlpPolicy**.","metadata":{"id":"g_JClfElGFnF"}},{"cell_type":"code","source":"print(\"\\n _____ACTION SPACE_____ \\n\")\nprint(\"The Action Space is: \", a_size)\nprint(\"Action Space Sample\", env.action_space.sample()) # Take a random action","metadata":{"id":"ib1Kxy4AF-FC","execution":{"iopub.status.busy":"2024-06-21T12:37:14.845155Z","iopub.execute_input":"2024-06-21T12:37:14.845943Z","iopub.status.idle":"2024-06-21T12:37:14.852360Z","shell.execute_reply.started":"2024-06-21T12:37:14.845904Z","shell.execute_reply":"2024-06-21T12:37:14.851117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The action space is a vector with 3 values:\n- Control x, y, z movement","metadata":{"id":"5MHTHEHZS4yp"}},{"cell_type":"markdown","source":"### Normalize observation and rewards","metadata":{"id":"S5sXcg469ysB"}},{"cell_type":"markdown","source":"A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).\n\nFor that purpose, there is a wrapper that will compute a running average and standard deviation of input features.\n\nWe also normalize rewards with this same wrapper by adding `norm_reward = True`\n\n[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)","metadata":{"id":"1ZyX6qf3Zva9"}},{"cell_type":"markdown","source":"#### Solution","metadata":{"id":"tF42HvI7-gs5"}},{"cell_type":"code","source":"env = make_vec_env(env_id, n_envs=4)\n\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)","metadata":{"id":"2O67mqgC-hol","execution":{"iopub.status.busy":"2024-06-21T16:04:17.895927Z","iopub.execute_input":"2024-06-21T16:04:17.896328Z","iopub.status.idle":"2024-06-21T16:04:18.457314Z","shell.execute_reply.started":"2024-06-21T16:04:17.896294Z","shell.execute_reply":"2024-06-21T16:04:18.455432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create the A2C Model ü§ñ\n\nFor more information about A2C implementation with StableBaselines3 check: https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html#notes\n\nTo find the best parameters I checked the [official trained agents by Stable-Baselines3 team](https://huggingface.co/sb3).","metadata":{"id":"4JmEVU6z1ZA-"}},{"cell_type":"markdown","source":"#### Solution","metadata":{"id":"nWAuOOLh-oQf"}},{"cell_type":"code","source":"# model = A2C(policy = \"MultiInputPolicy\",\n#             env = env,\n#             verbose=1,\n#             learning_rate=0.0008)\n\n# in case of previously trained model load\nmodel = A2C.load(\"a2c-PandaReachDense-v3\", env=env)","metadata":{"id":"FKFLY54T-pU1","execution":{"iopub.status.busy":"2024-06-21T16:04:21.987700Z","iopub.execute_input":"2024-06-21T16:04:21.988130Z","iopub.status.idle":"2024-06-21T16:04:22.004925Z","shell.execute_reply.started":"2024-06-21T16:04:21.988088Z","shell.execute_reply":"2024-06-21T16:04:22.003510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the A2C agent üèÉ\n- Let's train our agent for 1,000,000 timesteps, don't forget to use GPU on Colab. It will take approximately ~25-40min","metadata":{"id":"opyK3mpJ1-m9"}},{"cell_type":"code","source":"model.learn(1_500_000)","metadata":{"id":"4TuGHZD7RF1G","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-06-21T14:29:30.902954Z","iopub.execute_input":"2024-06-21T14:29:30.903362Z","iopub.status.idle":"2024-06-21T15:33:48.936806Z","shell.execute_reply.started":"2024-06-21T14:29:30.903329Z","shell.execute_reply":"2024-06-21T15:33:48.935408Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the model and  VecNormalize statistics when saving the agent\nmodel.save(\"a2c-PandaReachDense-v3\")\nenv.save(\"vec_normalize.pkl\")","metadata":{"id":"MfYtjj19cKFr","execution":{"iopub.status.busy":"2024-06-21T15:33:48.939276Z","iopub.execute_input":"2024-06-21T15:33:48.939745Z","iopub.status.idle":"2024-06-21T15:33:48.957318Z","shell.execute_reply.started":"2024-06-21T15:33:48.939681Z","shell.execute_reply":"2024-06-21T15:33:48.955810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate the agent üìà\n- Now that's our  agent is trained, we need to **check its performance**.\n- Stable-Baselines3 provides a method to do that: `evaluate_policy`","metadata":{"id":"01M9GCd32Ig-"}},{"cell_type":"code","source":"from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaReachDense-v3\")])\neval_env = VecNormalize.load(\"vec_normalize.pkl\", eval_env)\n\n# We need to override the render_mode\neval_env.render_mode = \"rgb_array\"\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\ndel model\n# Load the agent\nmodel = A2C.load(\"a2c-PandaReachDense-v3\")\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"id":"liirTVoDkHq3","execution":{"iopub.status.busy":"2024-06-21T15:47:30.690083Z","iopub.execute_input":"2024-06-21T15:47:30.690451Z","iopub.status.idle":"2024-06-21T15:47:30.926847Z","shell.execute_reply.started":"2024-06-21T15:47:30.690424Z","shell.execute_reply":"2024-06-21T15:47:30.925550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Publish your trained model on the Hub üî•\nNow that we saw we got good results after the training, we can publish our trained model on the Hub with one line of code.\n\nüìö The libraries documentation üëâ https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n","metadata":{"id":"44L9LVQaavR8"}},{"cell_type":"markdown","source":"By using `package_to_hub`, as we already mentionned in the former units, **you evaluate, record a replay, generate a model card of your agent and push it to the hub**.\n\nThis way:\n- You can **showcase our work** üî•\n- You can **visualize your agent playing** üëÄ\n- You can **share with the community an agent that others can use** üíæ\n- You can **access a leaderboard üèÜ to see how well your agent is performing compared to your classmates** üëâ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n","metadata":{"id":"MkMk99m8bgaQ"}},{"cell_type":"markdown","source":"To be able to share your model with the community there are three more steps to follow:\n\n1Ô∏è‚É£ (If it's not already done) create an account to HF ‚û° https://huggingface.co/join\n\n2Ô∏è‚É£ Sign in and then, you need to store your authentication token from the Hugging Face website.\n- Create a new token (https://huggingface.co/settings/tokens) **with write role**\n\n<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n\n- Copy the token\n- Run the cell below and paste the token","metadata":{"id":"JquRrWytA6eo"}},{"cell_type":"code","source":"notebook_login()\n!git config --global credential.helper store","metadata":{"id":"GZiFBBlzxzxY","execution":{"iopub.status.busy":"2024-06-21T13:18:44.175103Z","iopub.execute_input":"2024-06-21T13:18:44.175503Z","iopub.status.idle":"2024-06-21T13:18:45.264314Z","shell.execute_reply.started":"2024-06-21T13:18:44.175470Z","shell.execute_reply":"2024-06-21T13:18:45.262654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you don't want to use a Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`","metadata":{"id":"_tsf2uv0g_4p"}},{"cell_type":"markdown","source":"3Ô∏è‚É£ We're now ready to push our trained agent to the ü§ó Hub üî• using `package_to_hub()` function","metadata":{"id":"FGNh9VsZok0i"}},{"cell_type":"markdown","source":"For this environment, **running this cell can take approximately 10min**","metadata":{"id":"juxItTNf1W74"}},{"cell_type":"code","source":"from huggingface_sb3 import package_to_hub\n\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"hishamcse/a2c-{env_id}\", # Change the username\n    commit_message=\"PandaReach Solved\",\n)","metadata":{"id":"V1N8r8QVwcCE","execution":{"iopub.status.busy":"2024-06-21T15:47:37.186758Z","iopub.execute_input":"2024-06-21T15:47:37.187170Z","iopub.status.idle":"2024-06-21T15:52:26.092119Z","shell.execute_reply.started":"2024-06-21T15:47:37.187140Z","shell.execute_reply":"2024-06-21T15:52:26.090843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Some additional challenges üèÜ\nThe best way to learn **is to try things by your own**! Why not trying  `PandaPickAndPlace-v3`?\n\nIf you want to try more advanced tasks for panda-gym, you need to check what was done using **TQC or SAC** (a more sample-efficient algorithm suited for robotics tasks). In real robotics, you'll use a more sample-efficient algorithm for a simple reason: contrary to a simulation **if you move your robotic arm too much, you have a risk of breaking it**.\n\nPandaPickAndPlace-v1 (this model uses the v1 version of the environment): https://huggingface.co/sb3/tqc-PandaPickAndPlace-v1\n\nAnd don't hesitate to check panda-gym documentation here: https://panda-gym.readthedocs.io/en/latest/usage/train_with_sb3.html\n\nWe provide you the steps to train another agent (optional):\n\n1. Define the environment called \"PandaPickAndPlace-v3\"\n2. Make a vectorized environment\n3. Add a wrapper to normalize the observations and rewards. [Check the documentation](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)\n4. Create the A2C Model (don't forget verbose=1 to print the training logs).\n5. Train it for 1M Timesteps\n6. Save the model and  VecNormalize statistics when saving the agent\n7. Evaluate your agent\n8. Publish your trained model on the Hub üî• with `package_to_hub`\n","metadata":{"id":"G3xy3Nf3c2O1"}},{"cell_type":"markdown","source":"# PandaPickAndPlace-v3","metadata":{}},{"cell_type":"markdown","source":"### Solution (optional)","metadata":{"id":"sKGbFXZq9ikN"}},{"cell_type":"code","source":"# 1 - 2\nenv_id = \"PandaPickAndPlace-v3\"\n\n# Create the env\nenv = gym.make(env_id, render_mode='rgb_array')\n\n# Get the state space and action space\ns_size = env.observation_space.shape\na_size = env.action_space\n\nenv = make_vec_env(env_id, n_envs=4)\n\n# 3\nenv = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)\n\n# 4\nmodel = A2C(policy = \"MultiInputPolicy\",\n            env = env,\n            verbose=1)\n\n# # in case of previously trained model load\n# model = A2C.load(model_name, env=env)\n\n# 5\nmodel.learn(1_000_000)","metadata":{"id":"J-cC-Feg9iMm","execution":{"iopub.status.busy":"2024-06-21T16:06:13.407239Z","iopub.execute_input":"2024-06-21T16:06:13.407731Z","iopub.status.idle":"2024-06-21T16:22:38.871228Z","shell.execute_reply.started":"2024-06-21T16:06:13.407680Z","shell.execute_reply":"2024-06-21T16:22:38.868968Z"},"_kg_hide-output":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6\nmodel_name = \"a2c-PandaPickAndPlace-v3\";\nmodel.save(model_name)\nenv.save(\"vec_normalize_2.pkl\")\n\n# 7\nfrom stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n\n# Load the saved statistics\neval_env = DummyVecEnv([lambda: gym.make(\"PandaPickAndPlace-v3\")])\neval_env = VecNormalize.load(\"vec_normalize_2.pkl\", eval_env)\n\n#  do not update them at test time\neval_env.training = False\n# reward normalization is not needed at test time\neval_env.norm_reward = False\n\n# Load the agent\nmodel = A2C.load(model_name)\n\nmean_reward, std_reward = evaluate_policy(model, eval_env)\n\nprint(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")","metadata":{"id":"-UnlKLmpg80p","execution":{"iopub.status.busy":"2024-06-21T16:22:54.300017Z","iopub.execute_input":"2024-06-21T16:22:54.300424Z","iopub.status.idle":"2024-06-21T16:22:56.282565Z","shell.execute_reply.started":"2024-06-21T16:22:54.300391Z","shell.execute_reply":"2024-06-21T16:22:56.280409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 8\npackage_to_hub(\n    model=model,\n    model_name=f\"a2c-{env_id}\",\n    model_architecture=\"A2C\",\n    env_id=env_id,\n    eval_env=eval_env,\n    repo_id=f\"hishamcse/a2c-{env_id}\", # TODO: Change the username\n    commit_message=\"Initial commit\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T16:23:46.934211Z","iopub.execute_input":"2024-06-21T16:23:46.934669Z","iopub.status.idle":"2024-06-21T16:28:38.182858Z","shell.execute_reply.started":"2024-06-21T16:23:46.934638Z","shell.execute_reply":"2024-06-21T16:28:38.181542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See you on Unit 7! üî•\n## Keep learning, stay awesome ü§ó","metadata":{"id":"usatLaZ8dM4P"}}]}